{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_training_loop.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"h_RR0u2AoBVl","colab_type":"code","colab":{}},"source":["# Use this cell to change directory to one level above. \n","# The default directory is the \"content\" folder, and its parent directory \n","# was preferable as it made uploads more convenient for me\n","\n","# In terms of what I uploaded, they correspond to helper and model classes that \n","# I had previously written, and didn't want to copy into this notebook\n","\n","%cd .. \n","!ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVBOm90Bwqiu","colab_type":"code","colab":{}},"source":["# Use this cell to mount your drive; it is useful for storing any output files \n","# to a safe location.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmPJoSDJsISF","colab_type":"code","colab":{}},"source":["# Use this cell to establish a connection to the Google Cloud storage where \n","# your dataset is located. \n","\n","import json\n","import os\n","import pprint\n","import re\n","import time\n","import tensorflow as tf\n","\n","bucket = 'YOUR_BUCKET_NAME'\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","if 'COLAB_TPU_ADDR' in os.environ:\n","  print(\"Found colab tpu addr\\n\")\n","  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n","  \n","  # Upload credentials to TPU.\n","  with tf.Session(TF_MASTER) as sess:    \n","    with open('/content/adc.json', 'r') as f:\n","      auth_info = json.load(f)\n","    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n","  # Now credentials are set for all future sessions on this TPU.\n","else:\n","  TF_MASTER=''\n","\n","with tf.Session(TF_MASTER) as session:\n","  pprint.pprint(session.list_devices())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyDVfi5g2EY2","colab_type":"code","colab":{}},"source":["########################################################################\n","###################  VERY IMPORTANT  ###################################\n","# Use this cell to downgrade TensorFlow to version 1.13; the code is not \n","# compatible with 1.14\n","!pip install tensorflow==1.13.2\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XdNffsxzS_h","colab_type":"text"},"source":["##################### \n","\n","The cell below corresponds to the main component of this notebook. It is responsible for training the network and saving the results (weights, accuracy, etc.) to Google Drive. \n","\n","Weights are exported in .h5 format. The entire models are saved, but these were found to be either glitchy, buggy, or redundant with respect to the exported weights, and so were not used. Results are saved into a .csv file for processing and plotting at a later time. \n","\n","In terms of process flow, training-related tasks were performed largely using this notebook, and various experiments were conducted by commenting/uncommenting various code blocks according to my needs. Non-training tasks were performed locally since they did not require the TPU\n","\n","##################### "]},{"cell_type":"code","metadata":{"id":"okGrm4rG8w2W","colab_type":"code","outputId":"1d2584f8-1b86-4134-865d-3d3e5ed21521","executionInfo":{"status":"error","timestamp":1565733130567,"user_tz":240,"elapsed":110632,"user":{"displayName":"Mete Aykul","photoUrl":"https://lh3.googleusercontent.com/-R633JujG0-w/AAAAAAAAAAI/AAAAAAAAAL4/c_v1pncpYXk/s64/photo.jpg","userId":"16141573371150003489"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Below are import statements to use required frameworks\n","from __future__ import absolute_import, division, print_function\n","\n","import os\n","import sys\n","\n","# ----------------------------------------------------------- #\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","import traceback\t# for error tracebacks\n","import csv\n","\n","# Imports below are for files in the working directory\n","import food_classifier_models as models\n","import helper_funcs as helper\n","\n","## END IMPORTS\n","\n","\n","## \n","# Hyperparameters\n","NUM_EPOCHS = 60\n","# format: epoch to update learning rate at, multiplier (for step decay)\n","EPOCH_UPDATES = [[25, 0.002], [45, 0.0004]]\n","''' batch size is multiplied by 8 by the TPU, so a value of 64 actually \n","corresponds to 512. If receiving errors, try to ensure that all samples are \n","captured in the batch, and drop the remainder '''\n","BATCH_SIZE = 64\n","'''each entry in the shuffle buffer is approximately 320kB. 1000 elements\n","corresponds to ~320MB. Choose a size that matches the desired amount of \n","memory you'd like to use '''\n","SHUFFLE_BUFFER_SIZE = 6144\n","INITIAL_LEARNING_RATE = 0.01\n","LR_DECAY = 0.0005\n","MOMENTUM = 0.9\n","\n","'''Tailor the steps_per_epoch to suit the number of batched entries you'd \n","expect given your chosen image augmentation scheme\n","\n","In my case, I had 22 for my batch size for no augmentation, 44 with minimal, \n","and 200 with more augmentation, for UEC100'''\n","#STEPS_PER_EPOCH = 22 # uecfood100 22 44 200\n","STEPS_PER_EPOCH = 110 # uecfood256 55 110 497\n","#VALID_STEPS = 5 # validation steps for UEC100\n","VALID_STEPS = 10 # validation steps for UEC256\n","\n","# Convenience constants\n","MODEL = 0\t# 0 for mobilenetv2, 1 for densenet\n","USE_WIDESLICE = False\n","IS_TRAINED = False\n","\n","DATASET = 1\t\t# 0 for UEC100, 1 for UEC256\n","\n","\n","# Dataset paths\n","\n","''' This group of constants below is used to set the base directory for the \n","tfrecords files '''\n","GCS_BASE = 'gs://YOUR_BUCKET_NAME' # <------------- ENTER YOUR BUCKET NAME\n","UEC256_DIR = '/uecfood256_split' # <--- Folder path from base bucket\n","RESULTS_BASE = '/content/gdrive/My Drive/PATH_TO_RESULTS' \n","#                                 ^^^^^^^^^^^^^^ ENTER YOUR GDRIVE PATH\n","UEC_TFRECORD_SPLIT = ['/val0.tfrecords', '/val1.tfrecords', '/val2.tfrecords',\n","                      '/val3.tfrecords', '/val4.tfrecords']\n","\n","\n","''' The group below defines a set of constants whose values are assigned based\n","on the dataset (UEC100 or UEC256). It's to maintain one point of change '''\n","NUM_CATEGORY = None\n","DS_BASE = None\n","\n","\n","''' This if statement assigns constant values (typically for paths) based on \n","whether we're training on UEC100 or 256 '''\n","if (DATASET == 0):\n","  NUM_CATEGORY = 100\t# 100 different food classes for UEC100\n","  DS_BASE = GCS_BASE\n","elif (DATASET == 1):\n","  NUM_CATEGORY = 256\t# 256 different food classes for UEC256\n","  DS_BASE = GCS_BASE + UEC256_DIR\n","\n","\n","# Paths and file names\n","UEC100_MOBNET_WEIGHTS = 'mob100weights.h5'\n","UEC100_MOBNET = 'modelmobnet.h5'\n","UEC100_MOBNET_METRICS = 'metricsmobnet.csv'\n","\n","UEC256_MOBNET_WEIGHTS = 'mob256weights.h5'\n","UEC256_MOBNET = 'modelmobnet.h5'\n","UEC256_MOBNET_METRICS = 'metricsmobnet256.csv'\n","\n","UEC100_DENSE_WEIGHTS = 'dense100weights.h5'\n","UEC100_DENSE = 'modeldense.h5'\n","UEC100_DENSE_METRICS = 'metricsdense.csv'\n","\n","UEC256_DENSE_WEIGHTS = 'dense256weights.h5'\n","UEC256_DENSE = 'modeldense.h5'\n","UEC256_DENSE_METRICS = 'metricsdense256.csv'\n","\n","BASE_PATH_WEIGHTS = None\n","BASE_PATH_MODEL = None\n","BASE_PATH_METRICS = None\n","\n","if DATASET == 0:\n","  if MODEL == 0:\n","    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC100_MOBNET_WEIGHTS\n","    BASE_PATH_MODEL = RESULTS_BASE + UEC100_MOBNET\n","    BASE_PATH_METRICS = RESULTS_BASE + UEC100_MOBNET_METRICS\n","  elif MODEL == 1:\n","    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC100_DENSE_WEIGHTS\n","    BASE_PATH_MODEL = RESULTS_BASE + UEC100_DENSE\n","    BASE_PATH_METRICS = RESULTS_BASE + UEC100_DENSE_METRICS\n","    \n","elif DATASET == 1:\n","  if MODEL == 0:\n","    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC256_MOBNET_WEIGHTS\n","    BASE_PATH_MODEL = RESULTS_BASE + UEC256_MOBNET\n","    BASE_PATH_METRICS = RESULTS_BASE + UEC256_MOBNET_METRICS\n","  elif MODEL == 1:\n","    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC256_DENSE_WEIGHTS\n","    BASE_PATH_MODEL = RESULTS_BASE + UEC256_DENSE\n","    BASE_PATH_METRICS = RESULTS_BASE + UEC256_DENSE_METRICS\n","\n","\n","## END constant definitions\n","\n","\n","''' \n","This callback function updates the learning rate for the Keras fit method. \n","@param epoch : The current epoch\n","@param lr : The current learning rate. \n","@return updated_lr if updates are required; lr otherwise\n","''' \n","def update_lr(epoch, lr):\n","  for epoch_update in EPOCH_UPDATES:\n","    if epoch == epoch_update[0]:\n","      updated_lr = epoch_update[1]\n","      print(\"Updating lr: \", updated_lr)\n","      return float(updated_lr)\n","    \n","  return float(lr)\n","  \n","        \n","'''\n","This is a function wrapper for the training dataset, since keras-TPU did not \n","support direct feeding at the time of experimentation (around ~Summer 2019). \n","\n","It implements best practices according to the TensorFlow documentation\n","\n","@return training_dataset : The modified training dataset, including batching \n","and shuffles\n","'''\n","def input_dataset():\n","  # Append all dataset paths except the last one (which is used for validation)\n","  training_dataset_paths = [DS_BASE + UEC_TFRECORD_SPLIT[i] for i in range(len(UEC_TFRECORD_SPLIT) - 1)]\n","  training_dataset = tf.data.Dataset.list_files(training_dataset_paths)\n","  training_dataset = training_dataset.apply(tf.contrib.data.parallel_interleave(\n","      tf.data.TFRecordDataset, cycle_length=4, sloppy = True))\n","  # Parses each example (or entry) in the .tfrecords file\n","  training_dataset = training_dataset.map(helper.parse_tfrecord_example, num_parallel_calls=8)\n","  training_dataset = training_dataset.cache()\n","  # Apply augmentation. This example evaluates my third scheme\n","  training_dataset = training_dataset.map(helper.augment_image_three, num_parallel_calls=8)\n","  # My augmentation scheme returns a list of images, which needs to be unbatched\n","  training_dataset = training_dataset.apply(tf.data.experimental.unbatch())\t\n","  training_dataset = training_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration = True)\n","  \n","  # Re-batches the shuffled data, dropping remainders to avoid errors\n","  training_dataset = training_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","  training_dataset = training_dataset.prefetch(BATCH_SIZE)\n","  training_dataset = training_dataset.repeat()\n","  \n","  return training_dataset\n","\n","'''\n","This is a function wrapper for the validation dataset, similar to above.It \n","implements best practices according to the TensorFlow documentation\n","\n","@return validation_dataset : The modified validation dataset\n","'''\n","def valid_dataset():\n","  testing_dataset_path = DS_BASE + UEC_TFRECORD_SPLIT[-1]\t\n","  validation_dataset = tf.data.Dataset.list_files(testing_dataset_path)\n","  validation_dataset = validation_dataset.apply(tf.contrib.data.parallel_interleave(\n","      tf.data.TFRecordDataset, cycle_length=4, sloppy = True))\n","  validation_dataset = validation_dataset.map(helper.parse_tfrecord_example, num_parallel_calls=8)\n","  validation_dataset = validation_dataset.cache()\n","  # 2850 arbitrarily chosen based on manually-determined size of dataset. Note\n","  # that this is less efficient for UEC100, which has less entries; consider \n","  # using 2 different values. I wanted convenience, so left it as is\n","  validation_dataset = validation_dataset.shuffle(2850, reshuffle_each_iteration = True)\n","  validation_dataset = validation_dataset.apply(\n","      tf.data.experimental.map_and_batch(\n","        map_func=helper.resize_validation_image, \n","        batch_size=BATCH_SIZE, \n","        drop_remainder=True,\n","        num_parallel_calls=8\n","      ))\n","  validation_dataset = validation_dataset.prefetch(BATCH_SIZE)\n","  validation_dataset = validation_dataset.repeat()\n","  \n","  return validation_dataset\n","\n","\n","\n","'''\n","Main training loop. Manual changes are required to re-run with different \n","experimental settings, like model. \n","'''\n","if __name__ == \"__main__\":\n","  tf.keras.backend.clear_session() # destroys old graphs to clear clutter\n","\n","  print(\"tf version: \", tf.VERSION) # <-------- ENSURE VERSION 1.13.2\n","  print(\"keras version: \", tf.keras.__version__)\n","  \n","  if MODEL == 0:\n","    print(\"\\nCreating mobnet model...\\n\")\n","    model = models.MobNetVTwo(BATCH_SIZE, NUM_CATEGORY, USE_WIDESLICE)\n","    \n","    #############################\n","    # Load weights here if desired. Comment the portion out if you dont want to \n","    #############################\n","    #model.load_weights('mob100weights.h5')\n","    #model.load_weights('mob256weights.h5')\n","    \n","  elif MODEL == 1:\n","    print(\"\\nCreating dense model...\\n\")\n","    model = models.DenseNet(BATCH_SIZE, NUM_CATEGORY, USE_WIDESLICE)\n","\n","    #############################\n","    # Load weights here if desired. Comment the portion out if you dont want to \n","    #############################\n","    #model.load_weights('dense100weights.h5')\n","    #model.load_weights('dense256weights.h5')\n","  \n","  \n","  #############################\n","  # Use below to determine how much fine-tuning you wish to do \n","  #############################\n","  num_layers = len(model.layers)\n","  \n","  for i in range(num_layers):\n","    layer = model.layers[i]\n","    layer.trainable = True\n","    \n","#     threshold = int(0.8 * num_layers)\n","#     if i <= threshold:\n","#       layer.trainable = False\n","#     else:\n","#       layer.trainable = True\n","  \n","  \n","  \n","  # This prints the model summary for review. Uncomment if you'd like to see it\n","  # model.summary()\n","  \n","  # Creates a stochastic gradient descent optimizer for training. Experiment \n","  # with your own if you'd like\n","  optimizer = tf.keras.optimizers.SGD(lr=INITIAL_LEARNING_RATE, momentum=MOMENTUM, decay=LR_DECAY, nesterov=True)\n","  \n","  print(\"\\nModel built. Compiling model...\\n\")\n","\n","  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","  print ('TPU address is', tpu_address)\n","  \n","  model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer=optimizer,\n","    metrics=['categorical_accuracy']\n","  )\n","  \n","  \n","  # This is necessary to use the TPU from a Keras model\n","  model = tf.contrib.tpu.keras_to_tpu_model(\n","      model,\n","      strategy=tf.contrib.tpu.TPUDistributionStrategy(\n","          tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n","      )\n","  )\n","  \n","  # Learning rate callback to update the learning rate at each epoch. This \n","  # assigns the previously-defined callback function\n","  lr_updater_cbk = tf.keras.callbacks.LearningRateScheduler(update_lr)\n","  print(\"\\nModel compiled. Beginning training loop...\\n\")\n","  \n","  \n","  # Train the model and get the history\n","  history = model.fit(\n","    x=input_dataset,\n","    epochs=NUM_EPOCHS,\n","    verbose=2,\n","    callbacks=[lr_updater_cbk],\n","    validation_data=valid_dataset,\n","    steps_per_epoch=STEPS_PER_EPOCH,\n","    validation_steps=VALID_STEPS\n","  )\n","  \n","  print(\"\\nCompleted training. Saving history to drive\")\n","  \n","  # Saves history to gdrive for later processing\n","  with open(BASE_PATH_METRICS, 'w', newline='') as csv_file:\n","    writer = csv.writer(csv_file)\n","    for key, value in history.history.items():\n","      writer.writerow([key, value])\n","      \n","  csv_file.close()\n","  \n","  print(\"\\nSaving weights to drive.\")\n","  savepath = BASE_PATH_WEIGHTS\n","  model.save_weights(savepath, overwrite=True)\n","  print(\"\\nWeights saved.\")\n","  \n","  # Use below to save model if you'd like. I personally did not find much use \n","  # for it, and it may have been buggy for me too -- I do not recall anymore.\n","  \n","#   print(\"\\nAttempting to save model\")\n","#   model.save(\n","#     BASE_PATH_MODEL,\n","#     overwrite=True,\n","#     include_optimizer=True\n","#   )\n","  \n","  print(\"\\nSaved. Exiting program\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf version:  1.13.2\n","keras version:  2.2.4-tf\n","\n","Creating mobnet model...\n","\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","mobnetv2_in (InputLayer)        (512, 224, 224, 3)   0                                            \n","__________________________________________________________________________________________________\n","Conv1_pad (ZeroPadding2D)       (512, 225, 225, 3)   0           mobnetv2_in[0][0]                \n","__________________________________________________________________________________________________\n","Conv1 (Conv2D)                  (512, 112, 112, 48)  1296        Conv1_pad[0][0]                  \n","__________________________________________________________________________________________________\n","bn_Conv1 (BatchNormalizationV1) (512, 112, 112, 48)  192         Conv1[0][0]                      \n","__________________________________________________________________________________________________\n","Conv1_relu (ReLU)               (512, 112, 112, 48)  0           bn_Conv1[0][0]                   \n","__________________________________________________________________________________________________\n","expanded_conv_depthwise (Depthw (512, 112, 112, 48)  432         Conv1_relu[0][0]                 \n","__________________________________________________________________________________________________\n","expanded_conv_depthwise_BN (Bat (512, 112, 112, 48)  192         expanded_conv_depthwise[0][0]    \n","__________________________________________________________________________________________________\n","expanded_conv_depthwise_relu (R (512, 112, 112, 48)  0           expanded_conv_depthwise_BN[0][0] \n","__________________________________________________________________________________________________\n","expanded_conv_project (Conv2D)  (512, 112, 112, 24)  1152        expanded_conv_depthwise_relu[0][0\n","__________________________________________________________________________________________________\n","expanded_conv_project_BN (Batch (512, 112, 112, 24)  96          expanded_conv_project[0][0]      \n","__________________________________________________________________________________________________\n","block_1_expand (Conv2D)         (512, 112, 112, 144) 3456        expanded_conv_project_BN[0][0]   \n","__________________________________________________________________________________________________\n","block_1_expand_BN (BatchNormali (512, 112, 112, 144) 576         block_1_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_1_expand_relu (ReLU)      (512, 112, 112, 144) 0           block_1_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_1_pad (ZeroPadding2D)     (512, 113, 113, 144) 0           block_1_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_1_depthwise (DepthwiseCon (512, 56, 56, 144)   1296        block_1_pad[0][0]                \n","__________________________________________________________________________________________________\n","block_1_depthwise_BN (BatchNorm (512, 56, 56, 144)   576         block_1_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_1_depthwise_relu (ReLU)   (512, 56, 56, 144)   0           block_1_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_1_project (Conv2D)        (512, 56, 56, 32)    4608        block_1_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_1_project_BN (BatchNormal (512, 56, 56, 32)    128         block_1_project[0][0]            \n","__________________________________________________________________________________________________\n","block_2_expand (Conv2D)         (512, 56, 56, 192)   6144        block_1_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_2_expand_BN (BatchNormali (512, 56, 56, 192)   768         block_2_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_2_expand_relu (ReLU)      (512, 56, 56, 192)   0           block_2_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_2_depthwise (DepthwiseCon (512, 56, 56, 192)   1728        block_2_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_2_depthwise_BN (BatchNorm (512, 56, 56, 192)   768         block_2_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_2_depthwise_relu (ReLU)   (512, 56, 56, 192)   0           block_2_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_2_project (Conv2D)        (512, 56, 56, 32)    6144        block_2_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_2_project_BN (BatchNormal (512, 56, 56, 32)    128         block_2_project[0][0]            \n","__________________________________________________________________________________________________\n","block_2_add (Add)               (512, 56, 56, 32)    0           block_1_project_BN[0][0]         \n","                                                                 block_2_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_3_expand (Conv2D)         (512, 56, 56, 192)   6144        block_2_add[0][0]                \n","__________________________________________________________________________________________________\n","block_3_expand_BN (BatchNormali (512, 56, 56, 192)   768         block_3_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_3_expand_relu (ReLU)      (512, 56, 56, 192)   0           block_3_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_3_pad (ZeroPadding2D)     (512, 57, 57, 192)   0           block_3_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_3_depthwise (DepthwiseCon (512, 28, 28, 192)   1728        block_3_pad[0][0]                \n","__________________________________________________________________________________________________\n","block_3_depthwise_BN (BatchNorm (512, 28, 28, 192)   768         block_3_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_3_depthwise_relu (ReLU)   (512, 28, 28, 192)   0           block_3_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_3_project (Conv2D)        (512, 28, 28, 48)    9216        block_3_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_3_project_BN (BatchNormal (512, 28, 28, 48)    192         block_3_project[0][0]            \n","__________________________________________________________________________________________________\n","block_4_expand (Conv2D)         (512, 28, 28, 288)   13824       block_3_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_4_expand_BN (BatchNormali (512, 28, 28, 288)   1152        block_4_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_4_expand_relu (ReLU)      (512, 28, 28, 288)   0           block_4_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_4_depthwise (DepthwiseCon (512, 28, 28, 288)   2592        block_4_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_4_depthwise_BN (BatchNorm (512, 28, 28, 288)   1152        block_4_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_4_depthwise_relu (ReLU)   (512, 28, 28, 288)   0           block_4_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_4_project (Conv2D)        (512, 28, 28, 48)    13824       block_4_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_4_project_BN (BatchNormal (512, 28, 28, 48)    192         block_4_project[0][0]            \n","__________________________________________________________________________________________________\n","block_4_add (Add)               (512, 28, 28, 48)    0           block_3_project_BN[0][0]         \n","                                                                 block_4_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_5_expand (Conv2D)         (512, 28, 28, 288)   13824       block_4_add[0][0]                \n","__________________________________________________________________________________________________\n","block_5_expand_BN (BatchNormali (512, 28, 28, 288)   1152        block_5_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_5_expand_relu (ReLU)      (512, 28, 28, 288)   0           block_5_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_5_depthwise (DepthwiseCon (512, 28, 28, 288)   2592        block_5_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_5_depthwise_BN (BatchNorm (512, 28, 28, 288)   1152        block_5_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_5_depthwise_relu (ReLU)   (512, 28, 28, 288)   0           block_5_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_5_project (Conv2D)        (512, 28, 28, 48)    13824       block_5_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_5_project_BN (BatchNormal (512, 28, 28, 48)    192         block_5_project[0][0]            \n","__________________________________________________________________________________________________\n","block_5_add (Add)               (512, 28, 28, 48)    0           block_4_add[0][0]                \n","                                                                 block_5_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_6_expand (Conv2D)         (512, 28, 28, 288)   13824       block_5_add[0][0]                \n","__________________________________________________________________________________________________\n","block_6_expand_BN (BatchNormali (512, 28, 28, 288)   1152        block_6_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_6_expand_relu (ReLU)      (512, 28, 28, 288)   0           block_6_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_6_pad (ZeroPadding2D)     (512, 29, 29, 288)   0           block_6_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_6_depthwise (DepthwiseCon (512, 14, 14, 288)   2592        block_6_pad[0][0]                \n","__________________________________________________________________________________________________\n","block_6_depthwise_BN (BatchNorm (512, 14, 14, 288)   1152        block_6_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_6_depthwise_relu (ReLU)   (512, 14, 14, 288)   0           block_6_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_6_project (Conv2D)        (512, 14, 14, 88)    25344       block_6_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_6_project_BN (BatchNormal (512, 14, 14, 88)    352         block_6_project[0][0]            \n","__________________________________________________________________________________________________\n","block_7_expand (Conv2D)         (512, 14, 14, 528)   46464       block_6_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_7_expand_BN (BatchNormali (512, 14, 14, 528)   2112        block_7_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_7_expand_relu (ReLU)      (512, 14, 14, 528)   0           block_7_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_7_depthwise (DepthwiseCon (512, 14, 14, 528)   4752        block_7_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_7_depthwise_BN (BatchNorm (512, 14, 14, 528)   2112        block_7_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_7_depthwise_relu (ReLU)   (512, 14, 14, 528)   0           block_7_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_7_project (Conv2D)        (512, 14, 14, 88)    46464       block_7_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_7_project_BN (BatchNormal (512, 14, 14, 88)    352         block_7_project[0][0]            \n","__________________________________________________________________________________________________\n","block_7_add (Add)               (512, 14, 14, 88)    0           block_6_project_BN[0][0]         \n","                                                                 block_7_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_8_expand (Conv2D)         (512, 14, 14, 528)   46464       block_7_add[0][0]                \n","__________________________________________________________________________________________________\n","block_8_expand_BN (BatchNormali (512, 14, 14, 528)   2112        block_8_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_8_expand_relu (ReLU)      (512, 14, 14, 528)   0           block_8_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_8_depthwise (DepthwiseCon (512, 14, 14, 528)   4752        block_8_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_8_depthwise_BN (BatchNorm (512, 14, 14, 528)   2112        block_8_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_8_depthwise_relu (ReLU)   (512, 14, 14, 528)   0           block_8_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_8_project (Conv2D)        (512, 14, 14, 88)    46464       block_8_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_8_project_BN (BatchNormal (512, 14, 14, 88)    352         block_8_project[0][0]            \n","__________________________________________________________________________________________________\n","block_8_add (Add)               (512, 14, 14, 88)    0           block_7_add[0][0]                \n","                                                                 block_8_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_9_expand (Conv2D)         (512, 14, 14, 528)   46464       block_8_add[0][0]                \n","__________________________________________________________________________________________________\n","block_9_expand_BN (BatchNormali (512, 14, 14, 528)   2112        block_9_expand[0][0]             \n","__________________________________________________________________________________________________\n","block_9_expand_relu (ReLU)      (512, 14, 14, 528)   0           block_9_expand_BN[0][0]          \n","__________________________________________________________________________________________________\n","block_9_depthwise (DepthwiseCon (512, 14, 14, 528)   4752        block_9_expand_relu[0][0]        \n","__________________________________________________________________________________________________\n","block_9_depthwise_BN (BatchNorm (512, 14, 14, 528)   2112        block_9_depthwise[0][0]          \n","__________________________________________________________________________________________________\n","block_9_depthwise_relu (ReLU)   (512, 14, 14, 528)   0           block_9_depthwise_BN[0][0]       \n","__________________________________________________________________________________________________\n","block_9_project (Conv2D)        (512, 14, 14, 88)    46464       block_9_depthwise_relu[0][0]     \n","__________________________________________________________________________________________________\n","block_9_project_BN (BatchNormal (512, 14, 14, 88)    352         block_9_project[0][0]            \n","__________________________________________________________________________________________________\n","block_9_add (Add)               (512, 14, 14, 88)    0           block_8_add[0][0]                \n","                                                                 block_9_project_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_10_expand (Conv2D)        (512, 14, 14, 528)   46464       block_9_add[0][0]                \n","__________________________________________________________________________________________________\n","block_10_expand_BN (BatchNormal (512, 14, 14, 528)   2112        block_10_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_10_expand_relu (ReLU)     (512, 14, 14, 528)   0           block_10_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_10_depthwise (DepthwiseCo (512, 14, 14, 528)   4752        block_10_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_10_depthwise_BN (BatchNor (512, 14, 14, 528)   2112        block_10_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_10_depthwise_relu (ReLU)  (512, 14, 14, 528)   0           block_10_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","block_10_project (Conv2D)       (512, 14, 14, 136)   71808       block_10_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","block_10_project_BN (BatchNorma (512, 14, 14, 136)   544         block_10_project[0][0]           \n","__________________________________________________________________________________________________\n","block_11_expand (Conv2D)        (512, 14, 14, 816)   110976      block_10_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","block_11_expand_BN (BatchNormal (512, 14, 14, 816)   3264        block_11_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_11_expand_relu (ReLU)     (512, 14, 14, 816)   0           block_11_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_11_depthwise (DepthwiseCo (512, 14, 14, 816)   7344        block_11_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_11_depthwise_BN (BatchNor (512, 14, 14, 816)   3264        block_11_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_11_depthwise_relu (ReLU)  (512, 14, 14, 816)   0           block_11_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","block_11_project (Conv2D)       (512, 14, 14, 136)   110976      block_11_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","block_11_project_BN (BatchNorma (512, 14, 14, 136)   544         block_11_project[0][0]           \n","__________________________________________________________________________________________________\n","block_11_add (Add)              (512, 14, 14, 136)   0           block_10_project_BN[0][0]        \n","                                                                 block_11_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","block_12_expand (Conv2D)        (512, 14, 14, 816)   110976      block_11_add[0][0]               \n","__________________________________________________________________________________________________\n","block_12_expand_BN (BatchNormal (512, 14, 14, 816)   3264        block_12_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_12_expand_relu (ReLU)     (512, 14, 14, 816)   0           block_12_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_12_depthwise (DepthwiseCo (512, 14, 14, 816)   7344        block_12_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_12_depthwise_BN (BatchNor (512, 14, 14, 816)   3264        block_12_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_12_depthwise_relu (ReLU)  (512, 14, 14, 816)   0           block_12_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","block_12_project (Conv2D)       (512, 14, 14, 136)   110976      block_12_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","block_12_project_BN (BatchNorma (512, 14, 14, 136)   544         block_12_project[0][0]           \n","__________________________________________________________________________________________________\n","block_12_add (Add)              (512, 14, 14, 136)   0           block_11_add[0][0]               \n","                                                                 block_12_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","block_13_expand (Conv2D)        (512, 14, 14, 816)   110976      block_12_add[0][0]               \n","__________________________________________________________________________________________________\n","block_13_expand_BN (BatchNormal (512, 14, 14, 816)   3264        block_13_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_13_expand_relu (ReLU)     (512, 14, 14, 816)   0           block_13_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_13_pad (ZeroPadding2D)    (512, 15, 15, 816)   0           block_13_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_13_depthwise (DepthwiseCo (512, 7, 7, 816)     7344        block_13_pad[0][0]               \n","__________________________________________________________________________________________________\n","block_13_depthwise_BN (BatchNor (512, 7, 7, 816)     3264        block_13_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_13_depthwise_relu (ReLU)  (512, 7, 7, 816)     0           block_13_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","block_13_project (Conv2D)       (512, 7, 7, 224)     182784      block_13_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","block_13_project_BN (BatchNorma (512, 7, 7, 224)     896         block_13_project[0][0]           \n","__________________________________________________________________________________________________\n","block_14_expand (Conv2D)        (512, 7, 7, 1344)    301056      block_13_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","block_14_expand_BN (BatchNormal (512, 7, 7, 1344)    5376        block_14_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_14_expand_relu (ReLU)     (512, 7, 7, 1344)    0           block_14_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_14_depthwise (DepthwiseCo (512, 7, 7, 1344)    12096       block_14_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_14_depthwise_BN (BatchNor (512, 7, 7, 1344)    5376        block_14_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_14_depthwise_relu (ReLU)  (512, 7, 7, 1344)    0           block_14_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","block_14_project (Conv2D)       (512, 7, 7, 224)     301056      block_14_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","block_14_project_BN (BatchNorma (512, 7, 7, 224)     896         block_14_project[0][0]           \n","__________________________________________________________________________________________________\n","block_14_add (Add)              (512, 7, 7, 224)     0           block_13_project_BN[0][0]        \n","                                                                 block_14_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","block_15_expand (Conv2D)        (512, 7, 7, 1344)    301056      block_14_add[0][0]               \n","__________________________________________________________________________________________________\n","block_15_expand_BN (BatchNormal (512, 7, 7, 1344)    5376        block_15_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_15_expand_relu (ReLU)     (512, 7, 7, 1344)    0           block_15_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_15_depthwise (DepthwiseCo (512, 7, 7, 1344)    12096       block_15_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_15_depthwise_BN (BatchNor (512, 7, 7, 1344)    5376        block_15_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_15_depthwise_relu (ReLU)  (512, 7, 7, 1344)    0           block_15_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","block_15_project (Conv2D)       (512, 7, 7, 224)     301056      block_15_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","block_15_project_BN (BatchNorma (512, 7, 7, 224)     896         block_15_project[0][0]           \n","__________________________________________________________________________________________________\n","block_15_add (Add)              (512, 7, 7, 224)     0           block_14_add[0][0]               \n","                                                                 block_15_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","block_16_expand (Conv2D)        (512, 7, 7, 1344)    301056      block_15_add[0][0]               \n","__________________________________________________________________________________________________\n","block_16_expand_BN (BatchNormal (512, 7, 7, 1344)    5376        block_16_expand[0][0]            \n","__________________________________________________________________________________________________\n","block_16_expand_relu (ReLU)     (512, 7, 7, 1344)    0           block_16_expand_BN[0][0]         \n","__________________________________________________________________________________________________\n","block_16_depthwise (DepthwiseCo (512, 7, 7, 1344)    12096       block_16_expand_relu[0][0]       \n","__________________________________________________________________________________________________\n","block_16_depthwise_BN (BatchNor (512, 7, 7, 1344)    5376        block_16_depthwise[0][0]         \n","__________________________________________________________________________________________________\n","block_16_depthwise_relu (ReLU)  (512, 7, 7, 1344)    0           block_16_depthwise_BN[0][0]      \n","__________________________________________________________________________________________________\n","wideslc_conv (Conv2D)           (512, 220, 1, 160)   537760      mobnetv2_in[0][0]                \n","__________________________________________________________________________________________________\n","block_16_project (Conv2D)       (512, 7, 7, 448)     602112      block_16_depthwise_relu[0][0]    \n","__________________________________________________________________________________________________\n","wideslc_bn (BatchNormalizationV (512, 220, 1, 160)   640         wideslc_conv[0][0]               \n","__________________________________________________________________________________________________\n","block_16_project_BN (BatchNorma (512, 7, 7, 448)     1792        block_16_project[0][0]           \n","__________________________________________________________________________________________________\n","wideslc_relu (Activation)       (512, 220, 1, 160)   0           wideslc_bn[0][0]                 \n","__________________________________________________________________________________________________\n","Conv_1 (Conv2D)                 (512, 7, 7, 1792)    802816      block_16_project_BN[0][0]        \n","__________________________________________________________________________________________________\n","wideslc_mxpool (MaxPooling2D)   (512, 44, 1, 160)    0           wideslc_relu[0][0]               \n","__________________________________________________________________________________________________\n","Conv_1_bn (BatchNormalizationV1 (512, 7, 7, 1792)    7168        Conv_1[0][0]                     \n","__________________________________________________________________________________________________\n","wideslc_avgpool (AveragePooling (512, 2, 1, 160)     0           wideslc_mxpool[0][0]             \n","__________________________________________________________________________________________________\n","out_relu (ReLU)                 (512, 7, 7, 1792)    0           Conv_1_bn[0][0]                  \n","__________________________________________________________________________________________________\n","wideslc_flatten (Flatten)       (512, 320)           0           wideslc_avgpool[0][0]            \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (512, 1792)          0           out_relu[0][0]                   \n","__________________________________________________________________________________________________\n","wideslc_cnct (Concatenate)      (512, 2112)          0           wideslc_flatten[0][0]            \n","                                                                 global_average_pooling2d[0][0]   \n","__________________________________________________________________________________________________\n","final_dense (Dense)             (512, 256)           540928      wideslc_cnct[0][0]               \n","==================================================================================================\n","Total params: 5,443,040\n","Trainable params: 5,394,784\n","Non-trainable params: 48,256\n","__________________________________________________________________________________________________\n","\n","Model built. Compiling model...\n","\n","TPU address is grpc://10.121.207.10:8470\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.121.207.10:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4266266322944864649)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15360960032090662026)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3936121385777503236)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 6964472280977571444)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4255416380937855066)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8228867486734940619)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 15113742076621874639)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4352639750978077275)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 3966943032823849205)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6978734682746756822)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 4439372867062487438)\n","WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n","INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.8999999761581421, 'decay': 0.0005000000237487257, 'nesterov': True}\n","INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.8999999761581421, 'decay': 0.0005000000237487257, 'nesterov': True}\n","\n","Model compiled. Beginning training loop...\n","\n","Epoch 1/60\n","INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(512,), dtype=tf.int32, name=None), TensorSpec(shape=(512, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(512, 256), dtype=tf.float32, name=None)]\n","INFO:tensorflow:Overriding default placeholder.\n","INFO:tensorflow:Cloning SGD {'lr': 0.009999999776482582, 'momentum': 0.8999999761581421, 'decay': 0.0005000000237487257, 'nesterov': True}\n","INFO:tensorflow:Remapping placeholder for mobnetv2_in\n","INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7fc4115cc320> []\n","INFO:tensorflow:Started compiling\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-044b5fd3f902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALID_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m   )\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                                   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m                                   steps_per_epoch, validation_steps, **kwargs)\n\u001b[0m\u001b[1;32m   1533\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_to_infeed_manager_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m   def _pipeline_fit_loop(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop\u001b[0;34m(self, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m             epoch_logs=epoch_logs)\n\u001b[0m\u001b[1;32m   1717\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0;31m# Sample-wise fit loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_pipeline_fit_loop_step_wise\u001b[0;34m(self, ins, callbacks, steps_per_epoch, epochs, do_validation, val_inputs, val_targets, val_sample_weights, validation_steps, epoch_logs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;31m# Loop prologue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_step_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_step_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Function shouldn't return anything!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36mpipeline_run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1334\u001b[0m           next_input_tensors)\n\u001b[1;32m   1335\u001b[0m       next_tpu_model_ops = self._tpu_model_ops_for_input_specs(\n\u001b[0;32m-> 1336\u001b[0;31m           next_input_specs, next_step_infeed_manager)\n\u001b[0m\u001b[1;32m   1337\u001b[0m       \u001b[0minfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_infeed_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_tpu_model_ops_for_input_specs\u001b[0;34m(self, input_specs, infeed_manager)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                                                  infeed_manager)\n\u001b[1;32m   1170\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_model_compiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tpu_model_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compilation_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m_test_model_compiles\u001b[0;34m(self, tpu_model_ops)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_error_message\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m       raise RuntimeError('Compilation failed: {}'.format(\n\u001b[0;32m-> 1114\u001b[0;31m           proto.status_error_message))\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Compilation failed: Compilation failure: Ran out of memory in memory space hbm. Used 12.35G of 8.00G hbm. Exceeded hbm capacity by 4.35G.\n\nTotal hbm usage >= 12.35G:\n    reserved        528.00M \n    program          11.83G \n    arguments       unknown size \n\nOutput size unknown.\n\nProgram hbm requirement 11.83G:\n    reserved          12.0K\n    global           228.0K\n    scoped             5.0K\n    HLO temp         11.83G (99.6% utilization, 0.0% fragmentation (3.41M))\n\n  Largest program allocations in hbm:\n\n  1. Size: 3.45G\n     Operator: op_type=\"Slice\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_1_pad/Pad_grad/Slice_1\"\n     Shape: f32[512,112,112,144]{0,3,2,1:T(8,128)}\n     Unpadded size: 3.45G\n     XLA label: %fusion.6.remat5 = f32[512,112,112,144]{0,3,2,1:T(8,128)} fusion(f32[3,3,144,1]{2,3,1,0:T(2,128)} %copy.674.remat4, f32[144]{0:T(256)} %get-tuple-element.6267, f32[144]{0:T(256)} %get-tuple-element.6268, f32[144]{0:T(256)} %get-tuple-element.6134, f32[512,...\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 3.45G\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140480133123040/block_1_expand/Conv2D\"\n     Shape: f32[512,112,112,144]{0,3,2,1:T(8,128)}\n     Unpadded size: 3.45G\n     XLA label: %fusion.336.remat5 = f32[512,112,112,144]{0,3,2,1:T(8,128)} fusion(f32[1,1,24,144]{3,2,1,0:T(8,128)} %get-tuple-element.6264, f32[24]{0:T(256)} %get-tuple-element.6407, f32[24]{0:T(256)} %get-tuple-element.6406, f32[24]{0:T(256)} %get-tuple-element.6132, f...\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 1.15G\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_140480133123040/expanded_conv_depthwise_BN/FusedBatchNorm\"\n     Shape: f32[512,112,112,48]{0,3,2,1:T(8,128)}\n     Unpadded size: 1.15G\n     XLA label: %fusion.230 = (f32[48]{0:T(256)}, f32[48]{0:T(256)}, f32[512,112,112,48]{0,3,2,1:T(8,128)}) fusion(bf16[3,3,1,48]{3,2,1,0:T(4,128)(2,1)} %reshape.7.remat, f32[48]{0:T(256)} %get-tuple-element.6397, f32[48]{0:T(256)} %get-tuple-element.6396, f32[48]{0:T(256...\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 1.15G\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140480133123040/Conv1/Conv2D\"\n     Shape: f32[512,112,112,48]{0,3,2,1:T(8,128)}\n     Unpadded size: 1.15G\n     XLA label: %convolution.148.remat5 = f32[512,112,112,48]{0,3,2,1:T(8,128)} convolution(bf16[512,224,224,3]{0,3,2,1:T(4,128)(2,1)} %copy.654, f32[3,3,3,48]{3,2,1,0:T(4,128)} %get-tuple-element.6148), window={size=3x3 stride=2x2 pad=0_1x0_1}, dim_labels=b01f_01io->b01f...\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 882.00M\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_1_project/Conv2D_grad/Conv2DBackpropInput\"\n     Shape: f32[512,56,56,144]{0,3,2,1:T(8,128)}\n     Unpadded size: 882.00M\n     XLA label: %fusion.131.remat5 = f32[512,56,56,144]{0,3,2,1:T(8,128)} fusion(f32[1,1,144,32]{2,3,1,0:T(8,128)} %get-tuple-element.6269, f32[144]{0:T(256)} %get-tuple-element.6262, f32[144]{0:T(256)} %get-tuple-element.6263, f32[144]{0:T(256)} %get-tuple-element.6137, ...\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 882.00M\n     Operator: op_type=\"FusedBatchNorm\" op_name=\"tpu_140480133123040/block_1_depthwise_BN/FusedBatchNorm\"\n     Shape: f32[512,56,56,144]{0,3,2,1:T(8,128)}\n     Unpadded size: 882.00M\n     XLA label: %fusion.273 = (f32[144]{0:T(256)}, f32[144]{0:T(256)}, f32[512,56,56,144]{0,3,2,1:T(8,128)}) fusion(bf16[3,3,1,144]{3,2,1,0:T(4,128)(2,1)} %reshape.8.remat2, f32[144]{0:T(256)} %get-tuple-element.6268, f32[144]{0:T(256)} %get-tuple-element.6267, f32[144]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 588.00M\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140480133123040/expanded_conv_project/Conv2D\"\n     Shape: f32[512,112,112,24]{0,3,2,1:T(8,128)}\n     Unpadded size: 588.00M\n     XLA label: %fusion.280.remat6 = f32[512,112,112,24]{0,3,2,1:T(8,128)} fusion(f32[1,1,48,24]{2,3,1,0:T(8,128)} %get-tuple-element.6403, f32[48]{0:T(256)} %get-tuple-element.6402, f32[48]{0:T(256)} %get-tuple-element.6401, f32[48]{0:T(256)} %get-tuple-element.5109, f32...\n     Allocation type: HLO temp\n     ==========================\n\n  8. Size: 196.00M\n     Operator: op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"\n     Shape: bf16[512,224,224,3]{0,3,2,1:T(4,128)(2,1)}\n     Unpadded size: 147.00M\n     Extra memory due to padding: 49.00M (1.3x expansion)\n     XLA label: %copy.654 = bf16[512,224,224,3]{0,3,2,1:T(4,128)(2,1)} copy(bf16[512,224,224,3]{3,2,1,0:T(8,128)(2,1)} %reshape.44), metadata={op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"}\n     Allocation type: HLO temp\n     ==========================\n\n  9. Size: 68.75M\n     Operator: op_type=\"FusedBatchNormGrad\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/wideslc_bn/FusedBatchNorm_grad/FusedBatchNormGrad\"\n     Shape: f32[512,220,1,160]{0,3,2,1:T(8,128)}\n     Unpadded size: 68.75M\n     XLA label: %fusion.992 = (f32[160]{0:T(256)}, f32[160]{0:T(256)}, f32[512,220,1,160]{0,3,2,1:T(8,128)}) fusion(f32[160]{0:T(256)} %get-tuple-element.5954, f32[512,220,1,160]{0,3,2,1:T(8,128)} %select-and-scatter, f32[160]{0:T(256)} %get-tuple-element.6412, f32[160]{0...\n     Allocation type: HLO temp\n     ==========================\n\n  10. Size: 68.75M\n     Operator: op_type=\"MaxPoolGrad\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/wideslc_mxpool/MaxPool_grad/MaxPoolGrad\"\n     Shape: f32[512,220,1,160]{0,3,2,1:T(8,128)}\n     Unpadded size: 68.75M\n     XLA label: %select-and-scatter = f32[512,220,1,160]{0,3,2,1:T(8,128)} select-and-scatter(f32[512,220,1,160]{0,3,2,1:T(8,128)} %fusion.765, f32[512,44,1,160]{0,3,2,1:T(8,128)} %reduce-window.2, f32[]{:T(256)} %constant.3), window={size=1x5x1x1 stride=1x5x1x1}, select=...\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 1.20M\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_16_expand/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,224,1344]{3,2,1,0:T(8,128)}\n     Unpadded size: 1.15M\n     Extra memory due to padding: 56.0K (1.0x expansion)\n     XLA label: %fusion.1120.remat4 = f32[1,1,224,1344]{3,2,1,0:T(8,128)} fusion(f32[1344]{0:T(1024)} %get-tuple-element.4853, f32[512,7,7,1344]{0,3,2,1:T(8,128)} %get-tuple-element.6086, f32[1344]{0:T(1024)} %get-tuple-element.5183, f32[1344]{0:T(1024)} %get-tuple-elemen...\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 1.20M\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_15_project/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,1344,224]{2,3,1,0:T(8,128)}\n     Unpadded size: 1.15M\n     Extra memory due to padding: 56.0K (1.0x expansion)\n     XLA label: %fusion.890.remat4 = f32[1,1,1344,224]{2,3,1,0:T(8,128)} fusion(f32[1344]{0:T(1024)} %get-tuple-element.6233, f32[1344]{0:T(1024)} %get-tuple-element.6232, f32[1344]{0:T(1024)} %get-tuple-element.5187, f32[512,7,7,1344]{0,3,2,1:T(8,128)} %fusion.896.remat7...\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 1.20M\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_15_expand/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,224,1344]{3,2,1,0:T(8,128)}\n     Unpadded size: 1.15M\n     Extra memory due to padding: 56.0K (1.0x expansion)\n     XLA label: %fusion.1114 = f32[1,1,224,1344]{3,2,1,0:T(8,128)} fusion(f32[512,7,7,224]{0,3,2,1:T(8,128)} %get-tuple-element.6078, f32[1344]{0:T(1024)} %get-tuple-element.4851, f32[512,7,7,1344]{0,3,2,1:T(8,128)} %get-tuple-element.6088, f32[1344]{0:T(1024)} %get-tuple...\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 1.20M\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_14_project/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,1344,224]{2,3,1,0:T(8,128)}\n     Unpadded size: 1.15M\n     Extra memory due to padding: 56.0K (1.0x expansion)\n     XLA label: %fusion.882.remat4 = f32[1,1,1344,224]{2,3,1,0:T(8,128)} fusion(f32[1344]{0:T(1024)} %get-tuple-element.6218, f32[1344]{0:T(1024)} %get-tuple-element.6217, f32[1344]{0:T(1024)} %get-tuple-element.5193, f32[512,7,7,1344]{0,3,2,1:T(8,128)} %fusion.888.remat7...\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 1.20M\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_14_expand/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,224,1344]{3,2,1,0:T(8,128)}\n     Unpadded size: 1.15M\n     Extra memory due to padding: 56.0K (1.0x expansion)\n     XLA label: %fusion.1108 = f32[1,1,224,1344]{3,2,1,0:T(8,128)} fusion(f32[1344]{0:T(1024)} %get-tuple-element.4869, f32[512,7,7,1344]{0,3,2,1:T(8,128)} %get-tuple-element.6090, f32[1344]{0:T(1024)} %get-tuple-element.5195, f32[1344]{0:T(1024)} %get-tuple-element.6223,...\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 784.0K\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_13_project/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,816,224]{2,3,1,0:T(8,128)}\n     Unpadded size: 714.0K\n     Extra memory due to padding: 70.0K (1.1x expansion)\n     XLA label: %fusion.1002.remat4 = f32[1,1,816,224]{2,3,1,0:T(8,128)} fusion(f32[816]{0:T(1024)} %get-tuple-element.6203, f32[816]{0:T(1024)} %get-tuple-element.6202, f32[816]{0:T(1024)} %get-tuple-element.5199, f32[512,7,7,816]{0,3,2,1:T(8,128)} %fusion.556.remat7, f3...\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 636.0K\n     Shape: (f32[3,3,3,48]{3,2,1,0:T(4,128)}, f32[1792]{0:T(1024)}, f32[1792]{0:T(1024)}, f32[3,3,528,1]{2,3,1,0:T(2,128)}, f32[528]{0:T(1024)}, f32[528]{0:T(1024)}, f32[1,1,88,528]{3,2,1,0:T(8,128)}, f32[528]{0:T(1024)}, f32[528]{0:T(1024)}, f32[1,1,528,136]{2,3,1,0:T(8,128)}, f32[136]{0:T(256)}, f32[136]{0:T(256)}, f32[3,3,816,1]{2,3,1,0:T(2,128)}, f32[816]{0:T(1024)}, f32[816]{0:T(1024)}, f32[1,1,136,816]{3,2,1,0:T(8,128)}, f32[816]{0:T(1024)}, f32[816]{0:T(1024)}, f32[1,1,816,136]{2,3,1,0:T(8,128)}, f32[136]{0:T(256)}, f32[136]{0:T(256)}, f32[3,3,816,1]{2,3,1,0:T(2,128)}, f32[816]{0:T(1024)}, f32[816]{0:T(1024)}, f32[1,1,136,816]{3,2,1,0:T(8,128)}, f32[816]{0:T(1024)}, f32[816]{0:T(1024)}, f32[1,1,816,136]{2,3,1,0:T(8,128)}, f32[136]{0:T(256)}, f32[136]{0:T(256)}, f32[3,3,816,1]{2,3,1,0:T(2,128)}, f32[816]{0:T(1024)}, f32[816]{0:T(1024)}, f32[1,1,136,816]{3,2,1,0:T(8,128)}, f32[816]{0:T(1024)}, f32[816]{0:T(1024)}, f32[1,1,816,224]{2,3,1,0:T(8,128)}, f32[224]{0:T(256)}, f32[224]{0:T(256)}, f32[3,3,1344,1]{2,3,1,0:T(2,128)}, f32[1344]{0:T(1024)}, f32[1344]{0:T(1024)}, f32[1,1,224,1344]{3,2,1,0:T(8,128)}, f32[1344]{0:T(1024)}, f32[1344]{0:T(1024)}, f32[1,1,1344,224]{2,3,1,0:T(8,128)}, f32[224]{0:T(256)}, f32[224]{0:T(256)}, f32[3,3,1344,1]{2,3,1,0:T(2,128)}, f32[1344]{0:T(1024)}, f32[1344]{0:T(1024)}, f32[1,1,224,1344]{3,2,1,0:T(8,128)}, f32[1344]{0:T(1024)}, f32[1344]{0:T(1024)}, f32[1,1,1344,224]{2,3,1,0:T(8,128)}, f32[224]{0:T(256)}, f32[224]{0:T(256)}, f32[3,3,1344,1...\n     Unpadded size: 636.0K\n     XLA label: %all-reduce.172 = (f32[3,3,3,48]{3,2,1,0:T(4,128)}, f32[1792]{0:T(1024)}, f32[1792]{0:T(1024)}, f32[3,3,528,1]{2,3,1,0:T(2,128)}, f32[528]{0:T(1024)}, f32[528]{0:T(1024)}, f32[1,1,88,528]{3,2,1,0:T(8,128)}, f32[528]{0:T(1024)}, f32[528]{0:T(1024)}, f32[1,1...\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 476.0K\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_11_project/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,816,136]{2,3,1,0:T(8,128)}\n     Unpadded size: 433.5K\n     Extra memory due to padding: 42.5K (1.1x expansion)\n     XLA label: %fusion.558.remat3 = f32[1,1,816,136]{2,3,1,0:T(8,128)} fusion(f32[816]{0:T(1024)} %get-tuple-element.6173, f32[816]{0:T(1024)} %get-tuple-element.6172, f32[816]{0:T(1024)} %rsqrt.194.remat6, f32[512,14,14,816]{0,3,2,1:T(8,128)} %get-tuple-element.6074, f3...\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 476.0K\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_12_expand/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,136,816]{3,2,1,0:T(8,128)}\n     Unpadded size: 433.5K\n     Extra memory due to padding: 42.5K (1.1x expansion)\n     XLA label: %fusion.910 = f32[1,1,136,816]{3,2,1,0:T(8,128)} fusion(f32[512,14,14,136]{0,3,2,1:T(8,128)} %fusion.1039, f32[816]{0:T(1024)} %get-tuple-element.4789, f32[512,14,14,816]{0,3,2,1:T(8,128)} %get-tuple-element.6094, f32[816]{0:T(1024)} %get-tuple-element.516...\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 476.0K\n     Operator: op_type=\"Conv2DBackpropFilter\" op_name=\"training/KerasCrossShardOptimizer/gradients/tpu_140480133123040/block_12_project/Conv2D_grad/Conv2DBackpropFilter\"\n     Shape: f32[1,1,816,136]{2,3,1,0:T(8,128)}\n     Unpadded size: 433.5K\n     Extra memory due to padding: 42.5K (1.1x expansion)\n     XLA label: %fusion.566.remat4 = f32[1,1,816,136]{2,3,1,0:T(8,128)} fusion(f32[816]{0:T(1024)} %get-tuple-element.6188, f32[816]{0:T(1024)} %get-tuple-element.6187, f32[816]{0:T(1024)} %get-tuple-element.5167, f32[512,14,14,816]{0,3,2,1:T(8,128)} %get-tuple-element.60...\n     Allocation type: HLO temp\n     ==========================\n\n"]}]}]}