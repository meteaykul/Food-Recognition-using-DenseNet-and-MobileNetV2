{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_two_dense.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_RR0u2AoBVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this cell to change directory to the remote's root. \n",
        "# The default directory is the \"content\" folder, and the root directory \n",
        "# was preferable as it made uploads more convenient for me\n",
        "\n",
        "# In terms of what I uploaded, they correspond to helper and model classes that \n",
        "# I had previously written, and didn't want to copy into this notebook\n",
        "\n",
        "%cd .. \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVBOm90Bwqiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this cell to mount your drive; it is useful for storing any output files \n",
        "# to a safe location.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmPJoSDJsISF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this cell to establish a connection to the Google Cloud storage where \n",
        "# your dataset is located. \n",
        "\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import re\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "bucket = 'ENTER_BUCKET_NAME_HERE'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  print(\"Found colab tpu addr\\n\")\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "  \n",
        "  # Upload credentials to TPU.\n",
        "  with tf.Session(TF_MASTER) as sess:    \n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "else:\n",
        "  TF_MASTER=''\n",
        "\n",
        "with tf.Session(TF_MASTER) as session:\n",
        "  pprint.pprint(session.list_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdNffsxzS_h",
        "colab_type": "text"
      },
      "source": [
        "##################### \n",
        "\n",
        "The cell below corresponds to the main component of this notebook. It is responsible for training the network and saving the results (weights, accuracy, etc.) to Google Drive. \n",
        "\n",
        "Weights are exported in .h5 format. The entire models are saved, but these were found to be either glitchy, buggy, or redundant with respect to the exported weights, and so were not used. Results are saved into a .csv file for processing and plotting at a later time. \n",
        "\n",
        "In terms of process flow, training-related tasks were performed largely using this notebook, and various experiments were conducted by commenting/uncommenting various code blocks according to my needs. Non-training tasks were performed locally since they did not require the TPU\n",
        "\n",
        "##################### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okGrm4rG8w2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Below are import statements to use required frameworks\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ----------------------------------------------------------- #\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import traceback\t# for error tracebacks\n",
        "import csv\n",
        "\n",
        "# Imports below are for files in the working directory\n",
        "import food_classifier_models as models\n",
        "import helper_funcs as helper\n",
        "\n",
        "## END IMPORTS\n",
        "\n",
        "\n",
        "## \n",
        "# Hyperparameters\n",
        "NUM_EPOCHS = 60\n",
        "# format: epoch to update learning rate at, multiplier (for step decay)\n",
        "EPOCH_UPDATES = [[25, 0.002], [45, 0.0004]]\n",
        "''' batch size is multiplied by 8 by the TPU, so a value of 64 actually \n",
        "corresponds to 512. If receiving errors, try to ensure that all samples are \n",
        "captured in the batch, and drop the remainder '''\n",
        "BATCH_SIZE = 64\n",
        "'''each entry in the shuffle buffer is approximately 320kB. 1000 elements\n",
        "corresponds to ~320MB. Choose a size that matches the desired amount of \n",
        "memory you'd like to use '''\n",
        "SHUFFLE_BUFFER_SIZE = 6144\n",
        "INITIAL_LEARNING_RATE = 0.01\n",
        "LR_DECAY = 0.0005\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "'''Tailor the steps_per_epoch to suit the number of batched entries you'd \n",
        "expect given your chosen image augmentation scheme\n",
        "\n",
        "In my case, I had 22 for my batch size for no augmentation, 44 with minimal, \n",
        "and 200 with more augmentation, for UEC100'''\n",
        "#STEPS_PER_EPOCH = 22 # uecfood100 22 44 200\n",
        "STEPS_PER_EPOCH = 110 # uecfood256 55 110 497\n",
        "#VALID_STEPS = 5 # validation steps for UEC100\n",
        "VALID_STEPS = 10 # validation steps for UEC256\n",
        "\n",
        "# Convenience constants\n",
        "MODEL = 0\t# 0 for mobilenetv2, 1 for densenet\n",
        "USE_WIDESLICE = True\n",
        "IS_TRAINED = False\n",
        "\n",
        "DATASET = 1\t\t# 0 for UEC100, 1 for UEC256\n",
        "\n",
        "\n",
        "# Dataset paths\n",
        "\n",
        "''' This group of constants below is used to set the base directory for the \n",
        "tfrecords files '''\n",
        "GCS_BASE = 'gs://YOUR_PATH_TO_GOOGLE_STORAGE'\n",
        "UEC256_DIR = '/uecfood256_split'\n",
        "RESULTS_BASE = '/content/gdrive/My Drive/PATH_TO_RESULTS_DIRECTORY_IN_GDRIVE'\n",
        "UEC_TFRECORD_SPLIT = ['/val0.tfrecords', '/val1.tfrecords', '/val2.tfrecords',\n",
        "                      '/val3.tfrecords', '/val4.tfrecords']\n",
        "\n",
        "\n",
        "''' The group below defines a set of constants whose values are assigned based\n",
        "on the dataset (UEC100 or UEC256). It's to maintain one point of change '''\n",
        "NUM_CATEGORY = None\n",
        "DS_BASE = None\n",
        "\n",
        "\n",
        "''' This if statement assigns constant values (typically for paths) based on \n",
        "whether we're training on UEC100 or 256 '''\n",
        "if (DATASET == 0):\n",
        "  NUM_CATEGORY = 100\t# 100 different food classes for UEC100\n",
        "  DS_BASE = GCS_BASE\n",
        "elif (DATASET == 1):\n",
        "  NUM_CATEGORY = 256\t# 256 different food classes for UEC256\n",
        "  DS_BASE = GCS_BASE + UEC256_DIR\n",
        "\n",
        "\n",
        "# Paths and file names\n",
        "UEC100_MOBNET_WEIGHTS = 'mob100weights.h5'\n",
        "UEC100_MOBNET = 'modelmobnet.h5'\n",
        "UEC100_MOBNET_METRICS = 'metricsmobnet.csv'\n",
        "\n",
        "UEC256_MOBNET_WEIGHTS = 'mob256weights.h5'\n",
        "UEC256_MOBNET = 'modelmobnet.h5'\n",
        "UEC256_MOBNET_METRICS = 'metricsmobnet256.csv'\n",
        "\n",
        "UEC100_DENSE_WEIGHTS = 'dense100weights.h5'\n",
        "UEC100_DENSE = 'modeldense.h5'\n",
        "UEC100_DENSE_METRICS = 'metricsdense.csv'\n",
        "\n",
        "UEC256_DENSE_WEIGHTS = 'dense256weights.h5'\n",
        "UEC256_DENSE = 'modeldense.h5'\n",
        "UEC256_DENSE_METRICS = 'metricsdense256.csv'\n",
        "\n",
        "BASE_PATH_WEIGHTS = None\n",
        "BASE_PATH_MODEL = None\n",
        "BASE_PATH_METRICS = None\n",
        "\n",
        "if DATASET == 0:\n",
        "  if MODEL == 0:\n",
        "    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC100_MOBNET_WEIGHTS\n",
        "    BASE_PATH_MODEL = RESULTS_BASE + UEC100_MOBNET\n",
        "    BASE_PATH_METRICS = RESULTS_BASE + UEC100_MOBNET_METRICS\n",
        "  elif MODEL == 1:\n",
        "    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC100_DENSE_WEIGHTS\n",
        "    BASE_PATH_MODEL = RESULTS_BASE + UEC100_DENSE\n",
        "    BASE_PATH_METRICS = RESULTS_BASE + UEC100_DENSE_METRICS\n",
        "    \n",
        "elif DATASET == 1:\n",
        "  if MODEL == 0:\n",
        "    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC256_MOBNET_WEIGHTS\n",
        "    BASE_PATH_MODEL = RESULTS_BASE + UEC256_MOBNET\n",
        "    BASE_PATH_METRICS = RESULTS_BASE + UEC256_MOBNET_METRICS\n",
        "  elif MODEL == 1:\n",
        "    BASE_PATH_WEIGHTS = RESULTS_BASE + UEC256_DENSE_WEIGHTS\n",
        "    BASE_PATH_MODEL = RESULTS_BASE + UEC256_DENSE\n",
        "    BASE_PATH_METRICS = RESULTS_BASE + UEC256_DENSE_METRICS\n",
        "\n",
        "\n",
        "## END constant definitions\n",
        "\n",
        "\n",
        "''' \n",
        "This callback function updates the learning rate for the Keras fit method. \n",
        "@param epoch : The current epoch\n",
        "@param lr : The current learning rate. \n",
        "@return updated_lr if updates are required; lr otherwise\n",
        "''' \n",
        "def update_lr(epoch, lr):\n",
        "  for epoch_update in EPOCH_UPDATES:\n",
        "    if epoch == epoch_update[0]:\n",
        "      updated_lr = epoch_update[1]\n",
        "      print(\"Updating lr: \", updated_lr)\n",
        "      return float(updated_lr)\n",
        "    \n",
        "  return float(lr)\n",
        "  \n",
        "        \n",
        "'''\n",
        "This is a function wrapper for the training dataset, since keras-TPU did not \n",
        "support direct feeding at the time of experimentation (around ~Summer 2019). \n",
        "\n",
        "It implements best practices according to the TensorFlow documentation\n",
        "\n",
        "@return training_dataset : The modified training dataset, including batching \n",
        "and shuffles\n",
        "'''\n",
        "def input_dataset():\n",
        "  # Append all dataset paths except the last one (which is used for validation)\n",
        "  training_dataset_paths = [DS_BASE + UEC_TFRECORD_SPLIT[i] for i in range(len(UEC_TFRECORD_SPLIT) - 1)]\n",
        "  training_dataset = tf.data.Dataset.list_files(training_dataset_paths)\n",
        "  training_dataset = training_dataset.apply(tf.contrib.data.parallel_interleave(\n",
        "      tf.data.TFRecordDataset, cycle_length=4, sloppy = True))\n",
        "  # Parses each example (or entry) in the .tfrecords file\n",
        "  training_dataset = training_dataset.map(helper.parse_tfrecord_example, num_parallel_calls=8)\n",
        "  training_dataset = training_dataset.cache()\n",
        "  # Apply augmentation. This example evaluates my third scheme\n",
        "  training_dataset = training_dataset.map(helper.augment_image_three, num_parallel_calls=8)\n",
        "  # My augmentation scheme returns a list of images, which needs to be unbatched\n",
        "  training_dataset = training_dataset.apply(tf.data.experimental.unbatch())\t\n",
        "  training_dataset = training_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration = True)\n",
        "  \n",
        "  # Re-batches the shuffled data, dropping remainders to avoid errors\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  training_dataset = training_dataset.prefetch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  \n",
        "  return training_dataset\n",
        "\n",
        "'''\n",
        "This is a function wrapper for the validation dataset, similar to above.It \n",
        "implements best practices according to the TensorFlow documentation\n",
        "\n",
        "@return validation_dataset : The modified validation dataset\n",
        "'''\n",
        "def valid_dataset():\n",
        "  testing_dataset_path = DS_BASE + UEC_TFRECORD_SPLIT[-1]\t\n",
        "  validation_dataset = tf.data.Dataset.list_files(testing_dataset_path)\n",
        "  validation_dataset = validation_dataset.apply(tf.contrib.data.parallel_interleave(\n",
        "      tf.data.TFRecordDataset, cycle_length=4, sloppy = True))\n",
        "  validation_dataset = validation_dataset.map(helper.parse_tfrecord_example, num_parallel_calls=8)\n",
        "  validation_dataset = validation_dataset.cache()\n",
        "  # 2850 arbitrarily chosen based on manually-determined size of dataset. Note\n",
        "  # that this is less efficient for UEC100, which has less entries; consider \n",
        "  # using 2 different values. I wanted convenience, so left it as is\n",
        "  validation_dataset = validation_dataset.shuffle(2850, reshuffle_each_iteration = True)\n",
        "  validation_dataset = validation_dataset.apply(\n",
        "      tf.data.experimental.map_and_batch(\n",
        "        map_func=helper.resize_validation_image, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        drop_remainder=True,\n",
        "        num_parallel_calls=8\n",
        "      ))\n",
        "  validation_dataset = validation_dataset.prefetch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()\n",
        "  \n",
        "  return validation_dataset\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Main training loop. Manual changes are required to re-run with different \n",
        "experimental settings, like model. \n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "  tf.keras.backend.clear_session() # destroys old graphs to clear clutter\n",
        "\n",
        "  print(\"tf version: \", tf.VERSION)\n",
        "  print(\"keras version: \", tf.keras.__version__)\n",
        "  \n",
        "  if MODEL == 0:\n",
        "    print(\"\\nCreating mobnet model...\\n\")\n",
        "    model = models.MobNetVTwo(BATCH_SIZE, NUM_CATEGORY, USE_WIDESLICE)\n",
        "    \n",
        "    #############################\n",
        "    # Load weights here if desired. Comment the portion out if you dont want to \n",
        "    #############################\n",
        "    #model.load_weights('mob100weights.h5')\n",
        "    model.load_weights('mob256weights.h5')\n",
        "    \n",
        "  elif MODEL == 1:\n",
        "    print(\"\\nCreating dense model...\\n\")\n",
        "    model = models.DenseNet(BATCH_SIZE, NUM_CATEGORY, USE_WIDESLICE)\n",
        "\n",
        "    #############################\n",
        "    # Load weights here if desired. Comment the portion out if you dont want to \n",
        "    #############################\n",
        "    #model.load_weights('dense100weights.h5')\n",
        "    model.load_weights('dense256weights.h5')\n",
        "  \n",
        "  \n",
        "  #############################\n",
        "  # Use below to determine how much fine-tuning you wish to do \n",
        "  #############################\n",
        "  num_layers = len(model.layers)\n",
        "  \n",
        "  for i in range(num_layers):\n",
        "    layer = model.layers[i]\n",
        "    layer.trainable = True\n",
        "    \n",
        "#     threshold = int(0.8 * num_layers)\n",
        "#     if i <= threshold:\n",
        "#       layer.trainable = False\n",
        "#     else:\n",
        "#       layer.trainable = True\n",
        "  \n",
        "  \n",
        "  \n",
        "  # This prints the model summary for review. Uncomment if you'd like to see it\n",
        "  # model.summary()\n",
        "  \n",
        "  # Creates a stochastic gradient descent optimizer for training. Experiment \n",
        "  # with your own if you'd like\n",
        "  optimizer = tf.keras.optimizers.SGD(lr=INITIAL_LEARNING_RATE, momentum=MOMENTUM, decay=LR_DECAY, nesterov=True)\n",
        "  \n",
        "  print(\"\\nModel built. Compiling model...\\n\")\n",
        "\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "  \n",
        "  model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['categorical_accuracy']\n",
        "  )\n",
        "  \n",
        "  \n",
        "  # This is necessary to use the TPU from a Keras model\n",
        "  model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "      model,\n",
        "      strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "          tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "      )\n",
        "  )\n",
        "  \n",
        "  # Learning rate callback to update the learning rate at each epoch. This \n",
        "  # assigns the previously-defined callback function\n",
        "  lr_updater_cbk = tf.keras.callbacks.LearningRateScheduler(update_lr)\n",
        "  print(\"\\nModel compiled. Beginning training loop...\\n\")\n",
        "  \n",
        "  \n",
        "  # Train the model and get the history\n",
        "  history = model.fit(\n",
        "    x=input_dataset,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    verbose=2,\n",
        "    callbacks=[lr_updater_cbk],\n",
        "    validation_data=valid_dataset,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_steps=VALID_STEPS\n",
        "  )\n",
        "  \n",
        "  print(\"\\nCompleted training. Saving history to drive\")\n",
        "  \n",
        "  # Saves history to gdrive for later processing\n",
        "  with open(BASE_PATH_METRICS, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in history.history.items():\n",
        "      writer.writerow([key, value])\n",
        "      \n",
        "  csv_file.close()\n",
        "  \n",
        "  print(\"\\nSaving weights to drive.\")\n",
        "  savepath = BASE_PATH_WEIGHTS\n",
        "  model.save_weights(savepath, overwrite=True)\n",
        "  print(\"\\nWeights saved.\")\n",
        "  \n",
        "  # Use below to save model if you'd like. I personally did not find much use \n",
        "  # for it, and it may have been buggy for me too -- I do not recall anymore.\n",
        "  \n",
        "#   print(\"\\nAttempting to save model\")\n",
        "#   model.save(\n",
        "#     BASE_PATH_MODEL,\n",
        "#     overwrite=True,\n",
        "#     include_optimizer=True\n",
        "#   )\n",
        "  \n",
        "  print(\"\\nSaved. Exiting program\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}